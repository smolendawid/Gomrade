{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import tqdm\n",
    "import seaborn as sns\n",
    "import matplotlib.pylab as plt\n",
    "from keras.utils import to_categorical\n",
    "import pandas as pd\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import gc\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = '/kaggle/'\n",
    "main_path= '/Users/dasm/projects/Gomrade/'\n",
    "root = main_path + 'input/mygodataset/'\n",
    "root = main_path + 'data/'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dasm/venv/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (1028) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num classes: 3\n"
     ]
    }
   ],
   "source": [
    "X_MARGIN = 16\n",
    "Y_MARGIN = 16\n",
    "\n",
    "data = pd.read_csv(root + 'data.csv')\n",
    "columns = [str(i) for i in range(X_MARGIN*Y_MARGIN*2*2)]\n",
    "\n",
    "sources = data['source']\n",
    "examples = data['example']\n",
    "labels = data['label']\n",
    "image_inds = data['image_ind'].values.astype(np.uint16)\n",
    "data = data[columns].values.astype(np.uint8) #[:11000, :]\n",
    "\n",
    "labels = labels.values #[:11000]\n",
    "sources = sources.values #[:11000]\n",
    "examples = examples.values #[:11000]\n",
    "labels = to_categorical(labels)\n",
    "\n",
    "\n",
    "img_wh = X_MARGIN + Y_MARGIN\n",
    "num_channels = 1\n",
    "input_shape = (img_wh, img_wh, num_channels)\n",
    "# input_shape = (img_wh*img_wh,)\n",
    "num_classes = labels.shape[1]\n",
    "print(\"Num classes: {}\".format(num_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_shape = (img_wh*img_wh,)\n",
    "input_shape = (img_wh, img_wh, num_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_21 (Conv2D)           (None, 30, 30, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_22 (Conv2D)           (None, 13, 13, 64)        18496     \n",
      "_________________________________________________________________\n",
      "dropout_33 (Dropout)         (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_22 (MaxPooling (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 64)                147520    \n",
      "_________________________________________________________________\n",
      "dropout_34 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 166,531\n",
      "Trainable params: 166,531\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Importing the required Keras modules containing model and layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "# def base_model(img_w, img_h, input_shape, num_classes):\n",
    "\n",
    "#     model = Sequential()\n",
    "#     model.add(Conv2D(img_w, (3, 3), padding='same', activation='relu', input_shape=input_shape))\n",
    "#     model.add(Conv2D(img_w,(3, 3), activation='relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Conv2D(img_w*2, (3, 3), padding='same', activation='relu'))\n",
    "#     model.add(Conv2D(img_w*2, (3,3), activation='relu'))\n",
    "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#     model.add(Dropout(0.25))\n",
    "\n",
    "#     model.add(Flatten())\n",
    "#     model.add(Dense(512, activation='relu'))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(num_classes, activation='softmax'))\n",
    "#     optimizer = Adam(lr=0.001)\n",
    "#     return model, optimizer\n",
    "\n",
    "\n",
    "def base_model(img_w, img_h, input_shape, num_classes):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(int(img_w), kernel_size=(3,3), activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Conv2D(int(img_w*2), kernel_size=(3,3), activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
    "    model.add(Dense(img_w*2, activation=tf.nn.relu))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(num_classes, activation=tf.nn.softmax))\n",
    "    optimizer = Adam(lr=0.001)\n",
    "    return model, optimizer\n",
    "\n",
    "# def base_model(img_w, img_h, input_shape, num_classes):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(img_w*4, activation=tf.nn.relu, input_shape=input_shape))\n",
    "#     model.add(Dropout(0.4))\n",
    "#     model.add(Dense(num_classes, activation=tf.nn.softmax))\n",
    "#     optimizer = Adam(lr=0.001)\n",
    "#     return model, optimizer\n",
    "\n",
    "model, optimizer = base_model(img_wh, img_wh, input_shape, num_classes)\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def evaluate_time(model, x_test, y_test):\n",
    "    ex = x_test[:361]\n",
    "    start = time.time()\n",
    "    model.predict(ex)\n",
    "    model.predict(ex)\n",
    "    model.predict(ex)\n",
    "    model.predict(ex)\n",
    "    model.predict(ex)\n",
    "    print(\"Image inference: {}\".format((time.time() - start)/5))\n",
    "    \n",
    "    ex = x_test[:361*5]\n",
    "\n",
    "    start = time.time()\n",
    "    model.predict(ex)\n",
    "    print(\"Image inference: {}\".format((time.time() - start)/5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  import sys\n",
    "# # These are the usual ipython objects, including this one you are creating\n",
    "# ipython_vars = ['In', 'Out', 'exit', 'quit', 'get_ipython', 'ipython_vars']\n",
    "\n",
    "# # Get a sorted list of the objects and their sizes\n",
    "# sorted([(x, sys.getsizeof(globals().get(x))) for x in dir() if not x.startswith('_') and x not in sys.modules and x not in ipython_vars], key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_limit = 361*10\n",
    "accs = []\n",
    "accepted_sources = []\n",
    "all_y_test = []\n",
    "all_test_predictions = []\n",
    "all_y_train = []\n",
    "all_train_predictions = []\n",
    "all_correct = 0\n",
    "\n",
    "for source in set(sources):\n",
    "    if source == \"Potorti'-DeLazzari\" or source == \"20_03_29_16_33_37\" or source == \"DeLazzari-Greenberg\":\n",
    "        pass\n",
    "    else:\n",
    "        # 20_03_29_16_33_37 # DeLazzari-Greenberg\n",
    "        # 0.9459174275398254,0.9825117588043213,0.9658606052398682\n",
    "        continue\n",
    "    accepted_sources.append(source)\n",
    "    start = time.time()\n",
    "    print('\\n\\n')\n",
    "    print('='*100)\n",
    "    print('{}'.format(source))\n",
    "\n",
    "    test_ind = np.where(sources == source)[0]\n",
    "    train_ind = np.delete(np.arange(len(data)), test_ind)\n",
    "    if len(test_ind) < min_limit:\n",
    "        print('skipping {}'.format(source))\n",
    "        continue\n",
    "    \n",
    "    ex_test = examples[test_ind]\n",
    "    src_test = sources[test_ind]\n",
    "    imind_test = image_inds[test_ind]\n",
    "    \n",
    "    x_train = data[train_ind]/255\n",
    "    x_test = data[test_ind]/255\n",
    "    y_train = labels[train_ind]\n",
    "    y_test = labels[test_ind]\n",
    "    curr_inds = image_inds[test_ind]\n",
    "    \n",
    "    inds = np.where(np.argmax(y_train, axis=1) >0)[0]\n",
    "#     Reshaping the array to 4-dims so that it can work with the Keras API\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_wh, img_wh, num_channels)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_wh, img_wh, num_channels)\n",
    "\n",
    "    # Augmentation\n",
    "#     x_train = x_train.reshape(x_train.shape[0], img_wh, img_wh, num_channels)\n",
    "    x_train = np.vstack((x_train, x_train[inds, : ,::-1, :]))\n",
    "    y_train = np.vstack((y_train, y_train[inds]))\n",
    "                     \n",
    "#     x_train = x_train.reshape(x_train.shape[0], img_wh * img_wh)\n",
    "#     x_test = x_test.reshape(x_test.shape[0], img_wh * img_wh)\n",
    "    \n",
    "    # Making sure that the values are float so that we can get decimal points after division\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print('Number of images in x_train', x_train.shape[0])\n",
    "    print('Number of images in x_test', x_test.shape[0])\n",
    "\n",
    "    sns.countplot(np.argmax(y_train, axis=1))\n",
    "\n",
    "    class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(np.argmax(y_train, axis=1)),\n",
    "                                                 np.argmax(y_train, axis=1))\n",
    "    model, optimizer = base_model(img_wh, img_wh, input_shape, num_classes)\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "#     plot_losses = TrainingPlot(source)\n",
    "    history = model.fit(x_train, y_train, batch_size=128, epochs=1, verbose=False, validation_data=(x_test, y_test), class_weight=class_weights)\n",
    "    \n",
    "    loss, accuracy  = model.evaluate(x_test, y_test, verbose=False)\n",
    "    accs.append(accuracy)\n",
    "    \n",
    "    if accuracy == 1.0:\n",
    "        all_correct += 1\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['training', 'validation'], loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    print(f'Test loss: {loss:.3}')\n",
    "    print(f'Test accuracy: {accuracy:.3}')\n",
    "    \n",
    "    evaluate_time(model, x_test, y_test)\n",
    "    \n",
    "    predictions  = model.predict(x_test, verbose=False)\n",
    "    print(classification_report(np.argmax(y_test, axis=1), np.argmax(predictions, axis=1), digits=5))\n",
    "    \n",
    "    all_y_test.extend(np.argmax(y_test, axis=1).flatten().tolist())\n",
    "    all_test_predictions.extend(np.argmax(predictions, axis=1).flatten().tolist())\n",
    "\n",
    "    limit = 10\n",
    "    ea_ind = 0\n",
    "    for i, (ref, pred) in enumerate(zip(np.argmax(y_test, axis=1), np.argmax(predictions, axis=1))):\n",
    "        if ref != pred:\n",
    "            plt.figure(figsize=(4,4))\n",
    "            plt.imshow(x_test[i].reshape(X_MARGIN*2, Y_MARGIN*2))\n",
    "            plt.title(f'ref: {ref} \\npre: {pred}\\nsrc: {src_test[i]}\\nex: {ex_test[i]}\\nind {imind_test[i]}', family='monospace')\n",
    "            plt.savefig(f'{main_path}/working/ref_{ref}_pre_{pred}_src_{src_test[i]}_ex_{ex_test[i]}_ind_{imind_test[i]}.jpg')\n",
    "            ea_ind += 1\n",
    "        if ea_ind < limit:\n",
    "            plt.show()\n",
    "        else:\n",
    "            plt.close()\n",
    "                \n",
    "    predictions  = model.predict(x_train, verbose=False)\n",
    "    all_y_train.extend(np.argmax(y_train, axis=1).flatten().tolist())\n",
    "    all_train_predictions.extend(np.argmax(predictions, axis=1).flatten().tolist())\n",
    "        \n",
    "    print(time.time() - start)\n",
    "\n",
    "\n",
    "print(f'Full src acc: {all_correct/len(accepted_sources)*100}')\n",
    "print(f'Ave position accuracy: {np.mean(accs):.5}')\n",
    "print(f'Position std: {np.std(accs):.5}')\n",
    "print('Test:')\n",
    "print(classification_report(all_y_test, all_test_predictions, digits=5))\n",
    "print('Train:')\n",
    "print(classification_report(all_y_train, all_train_predictions, digits=5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.reset_metrics()\n",
    "model.save('go_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "373.66814279556274\n",
    "0.9944631088347662\n",
    "0.013440826207001617\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0    0.99823   0.99548   0.99685    296319\n",
    "           1    0.98055   0.99471   0.98758     31775\n",
    "           2    0.97602   0.98710   0.98153     31462\n",
    "\n",
    "    accuracy                        0.99468    359556\n",
    "   macro avg    0.98493   0.99243   0.98865    359556\n",
    "weighted avg    0.99472   0.99468   0.99469    359556\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0    0.99971   0.99974   0.99972   6175440\n",
    "           1    0.99942   0.99933   0.99938   1317410\n",
    "           2    0.99885   0.99879   0.99882   1305268\n",
    "\n",
    "    accuracy                        0.99954   8798118\n",
    "   macro avg    0.99933   0.99929   0.99931   8798118\n",
    "weighted avg    0.99954   0.99954   0.99954   8798118\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9852484464645386 20_03_29_16_33_37\n",
      "0.9761173725128174 DeLazzari-Greenberg\n",
      "0.9713758230209351 Potorti'-DeLazzari\n"
     ]
    }
   ],
   "source": [
    "for acc, src in zip(accs, accepted_sources):\n",
    "    print(acc, src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9972299337387085 20_03_29_16_33_37\n",
    "0.9798607230186462 DeLazzari-Greenberg\n",
    "0.9231631755828857 Potorti'-DeLazzari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.9965290427207947 20_03_29_16_33_37\n",
    "0.9826308488845825 DeLazzari-Greenberg\n",
    "0.9401134252548218 Potorti'-DeLazzari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.997430145740509 20_03_29_16_33_37\n",
    "0.9577749371528625 DeLazzari-Greenberg\n",
    "0.9872708320617676 Potorti'-DeLazzari"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
